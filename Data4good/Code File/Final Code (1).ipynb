{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th49bKulNdFX"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade predictionguard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGZ1rwdINi8Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "pg_access_token = getpass('Enter your Prediction Guard access token: ')\n",
        "os.environ['PREDICTIONGUARD_TOKEN'] = pg_access_token\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S7yjmHgNuAl"
      },
      "outputs": [],
      "source": [
        "import predictionguard as pg\n",
        "!pip install ijson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpcJImdKvggd"
      },
      "outputs": [],
      "source": [
        "pg.Completion.list_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kchoNOWdQGKS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "result = pg.Completion.create(\n",
        "    model=\"Nous-Hermes-Llama2-13B\",\n",
        "    prompt=\"Tell me a joke\"\n",
        ")\n",
        "\n",
        "print(json.dumps(\n",
        "    result,\n",
        "    sort_keys=True,\n",
        "    indent=4,\n",
        "    separators=(',', ': ')\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00OGB2E2RK8F"
      },
      "outputs": [],
      "source": [
        "result = pg.Factuality.check(\n",
        "    text=\"The sky is blue\",\n",
        "    reference=\"The sky is green\"\n",
        ")\n",
        "\n",
        "print(json.dumps(\n",
        "    result,\n",
        "    sort_keys=True,\n",
        "    indent=4,\n",
        "    separators=(',', ': ')\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ9d0GknBvlw"
      },
      "outputs": [],
      "source": [
        "result = pg.Completion.create(\n",
        "    model=\"Nous-Hermes-Llama2-13B\",\n",
        "    prompt=\"What is the sentiment of this statement: This is great!\",\n",
        "    output={\n",
        "        \"type\": \"categorical\",\n",
        "        \"categories\": [\"POS\", \"NEU\", \"NEG\"]\n",
        "    }\n",
        ")\n",
        "\n",
        "print(json.dumps(\n",
        "    result,\n",
        "    sort_keys=True,\n",
        "    indent=4,\n",
        "    separators=(',', ': ')\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrVl1wEAL8bA",
        "outputId": "6117179b-13a3-4d35-c4c8-77b6335fbba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# #Mount google drive. Run this cell as is\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# #Change working directory. Run this cell as is\n",
        "os.chdir(\"/content/drive/My Drive\")\n",
        "\n",
        "# #Training file path. Please change the file path when you run it in your system/colab. Create a folder under My Drive and upload training data there\n",
        "FilePath=\"/content/drive/My Drive/transcripts.json\"\n",
        "FilePath2=\"/content/drive/My Drive/test.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQQ7G2JOR_o5"
      },
      "outputs": [],
      "source": [
        "# Load the transcripts from the JSON file\n",
        "with open('transcripts.json', 'r', encoding='utf-8') as file:\n",
        "    transcripts = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF_Vanz9r1h7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Assuming 'test.csv' is your CSV file\n",
        "csv_file_path = 'test.csv'\n",
        "\n",
        "# Create a DataFrame named 'test' from the CSV file\n",
        "test = pd.read_csv(csv_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF0tGYboKeha"
      },
      "outputs": [],
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def visualize_json(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Convert JSON data to a single string for visualization\n",
        "    json_text = json.dumps(data, indent=2)\n",
        "\n",
        "    # Create a WordCloud from the JSON text\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(json_text)\n",
        "\n",
        "    # Plot the WordCloud as a text bubble\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "json_file_path = '/content/drive/My Drive/transcripts.json'\n",
        "visualize_json(json_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6PGSpBzCI4H"
      },
      "outputs": [],
      "source": [
        "# Choose the English transcript (e.g., transcript with key '0' as a string)\n",
        "selected_transcript = transcripts['1']\n",
        "\n",
        "# Create an LLM \"prompt\" that we will use to extract information from the transcript.\n",
        "prompt = f\"\"\"### Instruction:\n",
        "Your task is to parse an JSON containing the medical transcript. This should consist of the following information:\n",
        "Patient's Name:Look for any mention of the patient's name in the provided text in English, if no prediction return None.\n",
        "Patient's Age: Extract information about the age of the patient if available in English, if no prediction return None.\n",
        "Medical Condition:Identify and extract details about the patient's medical condition or health issue in English, if no prediction return None.\n",
        "Symptoms:Find and list any symptoms mentioned by the patient in English.\n",
        "Precautions:Look for recommendations or precautions given to the patient for managing their health in English.\n",
        "Medication/Drug: Identify and extract information about any medication or drug mentioned in the text in English.\n",
        "\n",
        "### Input:\n",
        "{selected_transcript}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Use Llama 2 to extract the information.\n",
        "result = pg.Completion.create(\n",
        "    model=\"Nous-Hermes-Llama2-13B\",\n",
        "    prompt=prompt\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "print(json.dumps(\n",
        "    result,\n",
        "    sort_keys=True,\n",
        "    indent=4,\n",
        "    separators=(',', ': ')\n",
        "))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCz1fkyaeG-8"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import uuid  # Import the uuid module\n",
        "from googletrans import Translator  # Import the Translator module\n",
        "\n",
        "# Function to process a batch of transcripts\n",
        "def process_batch(transcripts, prompt_template):\n",
        "    results = []\n",
        "\n",
        "    for transcript_key, transcript_text in transcripts.items():\n",
        "        # Create an LLM \"prompt\" for the current transcript\n",
        "        prompt = prompt_template.format(transcripts=transcript_text)\n",
        "\n",
        "        try:\n",
        "            # Use LLM model to extract information.\n",
        "            result = pg.Completion.create(\n",
        "                model=\"Nous-Hermes-Llama2-13B\",\n",
        "                prompt=prompt,\n",
        "                max_tokens=200  # Adjust max_tokens as needed\n",
        "            )\n",
        "\n",
        "            # Extract specific information from the result\n",
        "            text_info = result['choices'][0]['text']\n",
        "\n",
        "            # Add English translation\n",
        "            translator = Translator()\n",
        "            english_translation = translator.translate(text_info, src='auto', dest='en').text\n",
        "\n",
        "            # Split the text into rows\n",
        "            rows = english_translation.split('\\n')\n",
        "\n",
        "            # Append each row as a dictionary to the results list\n",
        "            for row in rows:\n",
        "                # Generate a unique ID for each row using uuid\n",
        "                row_id = str(uuid.uuid4())\n",
        "                # Extract information content\n",
        "                row_content = row.strip()\n",
        "                results.append({'Id': row_id, 'Text': row_content})\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Processed transcript {transcript_key}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Print the exception for debugging\n",
        "            print(f\"Error processing transcript {transcript_key}: {e}\")\n",
        "\n",
        "            # Write None when there is no prediction\n",
        "            row_id = str(uuid.uuid4())\n",
        "            results.append({'Id': row_id, 'Text': 'None'})\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Select the first 100 transcripts for testing\n",
        "subset_transcripts = {key: transcripts[key] for key in list(transcripts.keys())[:4917]}\n",
        "\n",
        "# Define the LLM prompt template\n",
        "prompt_template = \"\"\"### Instruction:\n",
        "Your task is to parse a JSON containing the medical transcript. This should consist of the following information:\n",
        "Patient's Name: Look for any mention of the patient's name in the provided text in English.\n",
        "Patient's Age: Extract information about the age of the patient if available in English.\n",
        "Medical Condition: Identify and extract details about the patient's medical condition or health issue in English.\n",
        "Symptoms: Find and list any symptoms mentioned by the patient in English.\n",
        "Precautions: Look for recommendations or precautions given to the patient for managing their health in English.\n",
        "Medication/Drug: Identify and extract information about any medication or drug mentioned in the text in English.\n",
        "\n",
        "### Input:\n",
        "{transcripts}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Initialize an empty list to store results\n",
        "all_results = []\n",
        "\n",
        "# Use ThreadPoolExecutor for asynchronous processing\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Process the transcripts concurrently\n",
        "    results = list(executor.map(lambda transcript: process_batch({transcript: subset_transcripts[transcript]}, prompt_template), subset_transcripts))\n",
        "\n",
        "    # Flatten the results\n",
        "    all_results = [result for batch_result in results for result in batch_result]\n",
        "\n",
        "    # Save all the results to a CSV file\n",
        "    csv_file_path = \"set_results.csv\"\n",
        "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        fieldnames = ['Id', 'Text']\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "        # Write header\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Write data\n",
        "        for result in all_results:\n",
        "            writer.writerow(result)\n",
        "\n",
        "print(f\"Results have been written to {csv_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8hITaShC7_YW"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import uuid\n",
        "from googletrans import Translator\n",
        "\n",
        "# Function to process a batch of transcripts\n",
        "def process_batch(transcripts, prompt_template):\n",
        "    results = []\n",
        "\n",
        "    for transcript_key, transcript_text in transcripts.items():\n",
        "        # Create an LLM \"prompt\" for the current transcript\n",
        "        prompt = prompt_template.format(transcripts=transcript_text)\n",
        "\n",
        "        try:\n",
        "            # Use LLM model to extract information.\n",
        "            result = pg.Completion.create(\n",
        "                model=\"Nous-Hermes-Llama2-13B\",\n",
        "                prompt=prompt,\n",
        "                max_tokens=200  # Adjust max_tokens as needed\n",
        "            )\n",
        "\n",
        "            # Extract specific information from the result\n",
        "            text_info = result['choices'][0]['text']\n",
        "\n",
        "            # Add English translation\n",
        "            translator = Translator()\n",
        "            english_translation = translator.translate(text_info, src='auto', dest='en').text\n",
        "\n",
        "            # Split the text into rows\n",
        "            rows = english_translation.split('\\n')\n",
        "\n",
        "            # Append each row as a dictionary to the results list\n",
        "            for row in rows:\n",
        "                # Generate a unique ID for each row using uuid\n",
        "                row_id = str(uuid.uuid4())\n",
        "                # Extract information content\n",
        "                row_content = row.strip()\n",
        "                results.append({'Id': row_id, 'Text': row_content})\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Processed transcript {transcript_key}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Print the exception for debugging\n",
        "            print(f\"Error processing transcript {transcript_key}: {e}\")\n",
        "\n",
        "            # Write None when there is no prediction\n",
        "            row_id = str(uuid.uuid4())\n",
        "            results.append({'Id': row_id, 'Text': 'None'})\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "# Select the first 100 transcripts for testing\n",
        "subset_transcripts = {key: transcripts[key] for key in list(transcripts.keys())[:4917]}\n",
        "\n",
        "# Define the LLM prompt template\n",
        "prompt_template = \"\"\"### Instruction:\n",
        "Your task is to parse a JSON containing the medical transcript. This should consist of the following information:\n",
        "Patient's Name: Look for any mention of the patient's name in the provided text in English.\n",
        "Patient's Age: Extract information about the age of the patient if available in English.\n",
        "Medical Condition: Identify and extract details about the patient's medical condition or health issue in English.\n",
        "Symptoms: Find and list any symptoms mentioned by the patient in English.\n",
        "Precautions: Look for recommendations or precautions given to the patient for managing their health in English.\n",
        "Medication/Drug: Identify and extract information about any medication or drug mentioned in the text in English.\n",
        "\n",
        "### Input:\n",
        "{transcripts}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Initialize an empty list to store results\n",
        "all_results = []\n",
        "\n",
        "# Process the transcripts one by one\n",
        "for transcript in subset_transcripts:\n",
        "    result = process_batch({transcript: subset_transcripts[transcript]}, prompt_template)\n",
        "    all_results.extend(result)\n",
        "\n",
        "# Save all the results to a CSV file\n",
        "csv_file_path = \"set_results.csv\"\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['Id', 'Text']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "    # Write header\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Write data\n",
        "    for result in all_results:\n",
        "        writer.writerow(result)\n",
        "\n",
        "print(f\"Results have been written to {csv_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "03rKQw9k-lzi",
        "outputId": "ee364e7d-da9b-481c-d5a2-d2e264cd8daf"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_81595cd3-dc41-413b-9e41-3accae2fcba7\", \"set_results.csv\", 294758)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Provide the filename\n",
        "filename = 'set_results.csv'\n",
        "\n",
        "# Download the file\n",
        "files.download(filename)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
